### model
model_name_or_path: /root/autodl-tmp/hf_models/Qwen1.5-1.8B-Chat
trust_remote_code: true


quantization_bit: 4
quantization_type: nf4      # 常见的QLoRA设置
double_quantization: true   # 常见的QLoRA设置
upcast_layernorm: false      # 之前日志建议开启

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: q_proj,v_proj
lora_alpha: 16                      # 通常是 lora_rank 的两倍

### dataset
dataset: hate_speech_sft
template: qwen
cutoff_len: 1280
max_samples: 4000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 8

### output
output_dir: saves/Qwen1.5-1.8B-Chat/sft_hate_speech
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: tensorboard  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
learning_rate: 5.0e-4
num_train_epochs: 20.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
eval_dataset: hate_speech_sft_dev
# val_size: 0.1
per_device_eval_batch_size: 8
eval_strategy: steps
eval_steps: 500


flash_attn: fa2


